ğŸ¤– AI Summarizer & Semantic Knowledge Base
Final Term Project â€“ Professional Training (Intermediate Level)
Course Code: Professional Training
Student ID: YN3012170034
Mode: Individual Project

ğŸ“– Project Overview
This project is a fully containerized Machine Learning application designed to perform advanced NLP tasks. It implements a microservices architecture to provide two core functionalities:

Abstractive Summarization: Condenses long articles into concise summaries using the DistilBART transformer model.
Semantic Vector Storage: Converts processed text into dense vector embeddings using MiniLM and stores them in ChromaDB for persistent history and semantic retrieval.
The entire solution is deployed using Docker and Docker Compose, ensuring a consistent environment capable of handling deep learning workloads.

ğŸ—ï¸ System Architecture
The application is split into three isolated containers communicating via a Docker Bridge Network:

Service	Technology	Internal Port	Mapped Host Port	Description
Frontend	Gradio (Python)	7860	7860	Interactive Web User Interface.
Backend	FastAPI (Python)	8000	8000	ML Inference Engine & Logic Layer.
Database	ChromaDB	8000	8001	Vector Database for storing embeddings.
ğŸ› ï¸ Tech Stack
Language: Python 3.9
Orchestration: Docker & Docker Compose
ML Frameworks: PyTorch (CPU Optimized), Hugging Face Transformers
Models:
Summarization: sshleifer/distilbart-cnn-12-6
Embeddings: sentence-transformers/all-MiniLM-L6-v2
Database: ChromaDB (Vector Store)
API: FastAPI
UI: Gradio
ğŸš€ Quick Start Guide
Prerequisites
Docker Desktop installed and running.
System RAM: Minimum 4GB allocated to Docker (Recommended: 6GB).
Windows WSL2 Users: Ensure your .wslconfig is set to memory=4GB.
Installation & Running
Clone or Unzip the project:

Bash

cd nlp_final_project
Build and Start the Containers:
(Note: The first run will take 5-10 minutes to download the 1.5GB ML models).

Bash

docker-compose up --build
Wait for Initialization:
Watch the terminal logs. Do not access the app until you see:

INFO: Application startup complete.

Access the Application:

Web Interface: Open http://localhost:7860 in your browser.
API Documentation: Open http://localhost:8000/docs to test endpoints directly.
Stop the Application:

Bash

docker-compose down
ğŸ“– Usage Guide
1. Summarizer Tab
Input: Paste a long news article or paragraph (e.g., about History or Technology) into the text box.
Action: Click the Summarize button.
Output: The system returns a condensed summary generated by the Transformer model.
Background: The text is simultaneously converted into a vector embedding and saved to the database.
2. Database History Tab
This feature demonstrates "Vector Persistence."

View Data: Click Refresh History.
Result: You will see the JSON data of your recent summaries retrieved directly from the ChromaDB container.
Verification: This confirms that the vector database is successfully storing the embeddings and metadata generated by the backend.
ğŸ“‚ Project Structure
text

nlp_final_project/
â”œâ”€â”€ docker-compose.yml       # Orchestration config for all 3 services
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ backend/                 # Backend Microservice
â”‚   â”œâ”€â”€ Dockerfile           # Optimized Python 3.9 image
â”‚   â”œâ”€â”€ main.py              # FastAPI app, DistilBART & MiniLM pipeline
â”‚   â””â”€â”€ requirements.txt     # PyTorch, Transformers, ChromaDB client
â””â”€â”€ frontend/                # Frontend Microservice
    â”œâ”€â”€ Dockerfile           # Python image for UI
    â”œâ”€â”€ app.py               # Gradio UI application logic
    â””â”€â”€ requirements.txt     # Gradio & Requests dependencies
ğŸ Troubleshooting
Issue	Solution
Connection Refused / Error	The Backend is still downloading models. Wait 2 minutes and try again.
Container Exits (Code 137)	Docker ran out of RAM. Increase Docker Memory to 4GB+.
History Error (ValueError)	Backend API has been patched to hide raw embedding vectors (fixed in main.py).
